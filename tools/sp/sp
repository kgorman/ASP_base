#!/usr/bin/env python3
"""
SP - Stream Processors CLI
Unified tool for WeatherFlow Atlas Stream Processing management
"""

import argparse
import json
import os
import sys
import time
        "timestamp": timestamp,
        "operation": "create_processors",
        "summary": {"total": 0, "success": 0, "failed": 0},
        "processors": []
    }
    
    processors_dir = Path(config_dir) / "processors"
    
    if not processors_dir.exists():
        result["processors"].append({
            "error": f"Processors directory not found: {processors_dir}",
            "operation": "create_processors",
            "status": "failed"
        })
        return result
    
    # Filter processor files based on processor_name if specified
    if processor_name:
        processor_files = [processors_dir / f"{processor_name}.json"]
        # Check if the specific file exists
        if not processor_files[0].exists():
            result["processors"].append({
                "error": f"Processor file not found: {processor_files[0]}",
                "operation": "create_processor",
                "status": "failed"
            })
            return result
    else:
        processor_files = list(processors_dir.glob("*.json"))
    
    if not processor_files:
        result["processors"].append({
            "error": f"No JSON files found in {processors_dir}",
            "operation": "create_processors", 
            "status": "failed"
        })
        return result
    
    for proc_file in processor_files:
        try:
            with open(proc_file, 'r') as f:
                processor_data = json.load(f)
            
            result["summary"]["total"] += 1
            
            # Extract processor name from filename (remove .json extension)
            processor_name = proc_file.stem
            
            # Get pipeline and options from JSON
            pipeline = processor_data.get("pipeline", [])
            options = processor_data.get("options")
            
            if not pipeline:
                proc_result = {
                    "name": processor_name,
                    "file": str(proc_file),
                    "operation": "create_processor",
                    "status": "failed",
                    "message": "No pipeline found in JSON file"
                }
            else:
                proc_result = api.create_processor_from_json(
                    name=processor_name,
                    pipeline=pipeline,
                    options=options
                )
                proc_result["file"] = str(proc_file)
            
            result["processors"].append(proc_result)
            if proc_result.get("status") == "created":
                result["summary"]["success"] += 1
            else:
                result["summary"]["failed"] += 1
                
        except Exception as e:
            result["summary"]["total"] += 1
            result["summary"]["failed"] += 1
            result["processors"].append({
                "name": proc_file.stem,
                "file": str(proc_file),
                "operation": "create_processor",
                "status": "failed",
                "message": f"Error reading file: {str(e)}"
            })
    
    return result


def test_processors(processor_name=None):
    """Test processor JSON files for validation"""
    timestamp = datetime.now(timezone.utc).isoformat()
    
    # Use the test runner script
    test_script = Path(__file__).parent / "test_runner.py"
    
    if not test_script.exists():
        return {
            "timestamp": timestamp,
            "operation": "test",
            "error": f"Test script not found: {test_script}",
            "message": "Run from the tools/sp directory or ensure test_runner.py exists"
        }
    
    try:
        # Build command - no need for --json since it's now the default
        cmd = [sys.executable, str(test_script)]
        if processor_name:
            cmd.extend(["--processor", processor_name])
        
        # Run the test script
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=Path(__file__).parent.parent.parent)
        
        # Since test_runner.py already outputs colorized JSON, just return it directly
        if result.returncode == 0:
            # The output is already colorized JSON, so we just need to print it
            # and return a simple success indicator for the sp command
            print(result.stdout)
            return {
                "timestamp": timestamp,
                "operation": "test",
                "success": True,
                "message": "Test completed successfully"
            }
        else:
            # Handle error case - test_runner.py failed
            print(result.stdout if result.stdout else result.stderr)
            return {
                "timestamp": timestamp,
                "operation": "test",
                "success": False,
                "exit_code": result.returncode,
                "error": "Test validation failed"
            }
        
    except Exception as e:
        return {
            "timestamp": timestamp,
            "operation": "test",
            "error": str(e),
            "success": False
        }


def run_connection_test(require_mongodb=True, specific_connection=None):
    """Run connection test with MongoDB native driver verification."""
    import os
    
    # Determine the path to the test script
    script_dir = Path(__file__).parent
    test_script = script_dir.parent / "test" / "test_connection.py"
    
    if not test_script.exists():
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "operation": "connection_test",
            "error": f"Test script not found: {test_script}",
            "success": False
        }
    
    # Check for virtual environment
    venv_dir = script_dir.parent / "venv"
    if venv_dir.exists():
        # Use virtual environment python
        python_exe = venv_dir / "bin" / "python3"
        if not python_exe.exists():
            python_exe = venv_dir / "bin" / "python"
    else:
        python_exe = "python3"
    
    # Check if MongoDB connection string is available when required
    if require_mongodb and not os.environ.get('MONGODB_CONNECTION_STRING'):
        print("CRITICAL: MongoDB native driver verification is REQUIRED")
        print("   Set MONGODB_CONNECTION_STRING environment variable with your Atlas cluster credentials.")
        print("   Example: export MONGODB_CONNECTION_STRING='mongodb+srv://user:pass@kgshardedcluster01.mongodb.net/'")
        print("\n   This ensures authoritative verification that your Stream Processing connection works.")
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "operation": "connection_test",
            "error": "MONGODB_CONNECTION_STRING environment variable required",
            "success": False
        }
    
    try:
        # Run the test
        print("Launching Atlas Stream Processing Connection Test")
        print("MongoDB Native Driver Verification: REQUIRED")
        print("=" * 60)
        
        result = subprocess.run([
            str(python_exe), str(test_script)
        ], check=False, cwd=str(script_dir.parent))
        
        success = result.returncode == 0
        
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "operation": "connection_test", 
            "success": success,
            "exit_code": result.returncode,
            "mongodb_verification": "required" if require_mongodb else "optional"
        }
        
    except Exception as e:
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "operation": "connection_test",
            "error": str(e),
            "success": False
        }


def main():
    parser = argparse.ArgumentParser(description="SP - Stream Processors CLI")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Workspace management commands
    workspaces_parser = subparsers.add_parser("workspaces", help="Stream Processing workspace management")
    workspaces_subparsers = workspaces_parser.add_subparsers(dest="workspace_action", help="Workspace actions")
    
    # Legacy instances alias for backward compatibility
    instances_parser = subparsers.add_parser("instances", help="Stream Processing workspace management (legacy alias)")
    instances_subparsers = instances_parser.add_subparsers(dest="instance_action", help="Workspace actions")
    
    # Workspace list
    workspaces_list_parser = workspaces_subparsers.add_parser("list", help="List Stream Processing workspaces")
    instances_list_parser = instances_subparsers.add_parser("list", help="List Stream Processing workspaces")
    
    # Workspace create
    workspaces_create_parser = workspaces_subparsers.add_parser("create", help="Create Stream Processing workspace")
    workspaces_create_parser.add_argument("name", help="Workspace name")
    workspaces_create_parser.add_argument("--cloud-provider", default="AWS", choices=["AWS", "GCP", "AZURE"], help="Cloud provider (default: AWS)")
    workspaces_create_parser.add_argument("--region", default="US_EAST_1", help="Region (default: US_EAST_1)")
    
    instances_create_parser = instances_subparsers.add_parser("create", help="Create Stream Processing workspace")
    instances_create_parser.add_argument("name", help="Workspace name")
    instances_create_parser.add_argument("--cloud-provider", default="AWS", choices=["AWS", "GCP", "AZURE"], help="Cloud provider (default: AWS)")
    instances_create_parser.add_argument("--region", default="US_EAST_1", help="Region (default: US_EAST_1)")
    
    # Workspace delete
    workspaces_delete_parser = workspaces_subparsers.add_parser("delete", help="Delete Stream Processing workspace") 
    workspaces_delete_parser.add_argument("name", help="Workspace name")
    
    instances_delete_parser = instances_subparsers.add_parser("delete", help="Delete Stream Processing workspace") 
    instances_delete_parser.add_argument("name", help="Workspace name")
    
    # Workspace details
    workspaces_details_parser = workspaces_subparsers.add_parser("details", help="Get Stream Processing workspace details")
    workspaces_details_parser.add_argument("name", help="Workspace name")
    
    instances_details_parser = instances_subparsers.add_parser("details", help="Get Stream Processing workspace details")
    instances_details_parser.add_argument("name", help="Workspace name")
    
    # Workspace connections management
    workspaces_connections_parser = workspaces_subparsers.add_parser("connections", help="Manage workspace connections")
    workspaces_connections_subparsers = workspaces_connections_parser.add_subparsers(dest="connections_action", help="Connection actions")
    
                                                   help="Require MongoDB native driver verification (default: True)")
    workspaces_connections_test_parser.add_argument("--connection", help="Test specific connection (default: first connection)")
    
    instances_connections_parser = instances_subparsers.add_parser("connections", help="Manage workspace connections")
    instances_connections_subparsers = instances_connections_parser.add_subparsers(dest="connections_action", help="Connection actions")
    
    # Instance connections create
    instances_connections_create_parser = instances_connections_subparsers.add_parser("create", help="Create connections from JSON files")
    
    # Instance connections list
    instances_connections_list_parser = instances_connections_subparsers.add_parser("list", help="List connections in instance")
    
    # Instance connections delete
    instances_connections_delete_parser = instances_connections_subparsers.add_parser("delete", help="Delete connection from instance")
    instances_connections_delete_parser.add_argument("connection_name", help="Connection name to delete")
    
    # Instance connections test
    instances_connections_test_parser = instances_connections_subparsers.add_parser("test", help="Test connections with authoritative MongoDB verification")
    instances_connections_test_parser.add_argument("--require-mongodb", action="store_true", default=True, 
                                                   help="Require MongoDB native driver verification (default: True)")
    instances_connections_test_parser.add_argument("--connection", help="Test specific connection (default: first connection)")
    
    # Processor management commands
    processors_parser = subparsers.add_parser("processors", help="Processor management")
    processors_subparsers = processors_parser.add_subparsers(dest="processor_action", help="Processor actions")
    
    # Processor create
    processors_create_parser = processors_subparsers.add_parser("create", help="Create processors from JSON files")
    processors_create_parser.add_argument("-p", "--processor", help="Specific processor name to create")
    
    # Processor list
    processors_list_parser = processors_subparsers.add_parser("list", help="List processors with full details (pipeline, stats, errors)")
    processors_list_parser.add_argument("-p", "--processor", help="Show specific processor")
    
    # Processor stats
    processors_stats_parser = processors_subparsers.add_parser("stats", help="Show processor statistics")
    processors_stats_parser.add_argument("-p", "--processor", help="Show stats for specific processor")
    processors_stats_parser.add_argument("--verbose", action="store_true", help="Show detailed statistics for each pipeline operator")
    
    # Processor start
    processors_start_parser = processors_subparsers.add_parser("start", help="Start processors")
    processors_start_parser.add_argument("-p", "--processor", help="Start specific processor")
    processors_start_parser.add_argument("-t", "--tier", help="Tier to start processor on (e.g., SP5, SP10, SP30, SP50)")
    processors_start_parser.add_argument("--all-tier", help="Tier to start all processors on (e.g., SP5, SP10, SP30, SP50)")
    processors_start_parser.add_argument("--auto", action="store_true", help="Automatically determine optimal tier based on processor complexity")
    
    # Processor stop
    processors_stop_parser = processors_subparsers.add_parser("stop", help="Stop processors")
    processors_stop_parser.add_argument("-p", "--processor", help="Stop specific processor")
    
    # Processor restart
    processors_restart_parser = processors_subparsers.add_parser("restart", help="Restart processors")
    processors_restart_parser.add_argument("-p", "--processor", help="Restart specific processor")
    processors_restart_parser.add_argument("-t", "--tier", help="Tier to restart processor on (e.g., SP5, SP10, SP30, SP50)")
    processors_restart_parser.add_argument("--all-tier", help="Tier to restart all processors on (e.g., SP5, SP10, SP30, SP50)")
    processors_restart_parser.add_argument("--auto", action="store_true", help="Automatically determine optimal tier based on processor complexity")
    
    # Processor drop
    processors_drop_parser = processors_subparsers.add_parser("drop", help="Drop (delete) processors")
    processors_drop_parser.add_argument("-p", "--processor", help="Drop specific processor")
    processors_drop_parser.add_argument("processor_name", nargs="?", help="Processor name to drop")
    processors_drop_parser.add_argument("--all", action="store_true", help="Drop all processors")
    
    # Processor test
    processors_test_parser = processors_subparsers.add_parser("test", help="Test processor configurations")
    processors_test_parser.add_argument("-p", "--processor", help="Test specific processor")
    
    # Processor tier-advise
    processors_tier_advise_parser = processors_subparsers.add_parser("tier-advise", help="Analyze processor and recommend optimal tier")
    processors_tier_advise_parser.add_argument("-p", "--processor", help="Analyze specific processor")
    processors_tier_advise_parser.add_argument("--all", action="store_true", help="Analyze all processors")
    
    # Processor profile
    processors_profile_parser = processors_subparsers.add_parser("profile", help="Profile processor performance over time")
    processors_profile_parser.add_argument("-p", "--processor", help="Profile specific processor")
    processors_profile_parser.add_argument("--all", action="store_true", help="Profile all processors")
    processors_profile_parser.add_argument("--duration", type=int, default=300, help="Profiling duration in seconds (default: 300)")
    processors_profile_parser.add_argument("--interval", type=int, default=30, help="Sampling interval in seconds (default: 30)")
    processors_profile_parser.add_argument("--metrics", default="memory,latency,throughput", help="Comma-separated metrics to track")
    processors_profile_parser.add_argument("--output", help="Output file for results (JSON format)")
    processors_profile_parser.add_argument("--continuous", action="store_true", help="Continuous monitoring (Ctrl+C to stop)")
    processors_profile_parser.add_argument("--thresholds", help="JSON file with alerting thresholds")
    
    # Processor schema (sample output)
    processors_schema_parser = processors_subparsers.add_parser("schema", help="Sample processor output to show schema")
    processors_schema_parser.add_argument("processor_name", help="Processor name to sample")
    processors_schema_parser.add_argument("-n", "--num-samples", type=int, default=3, help="Number of sample documents (default: 3)")
    
    # Collection management commands
    collections_parser = subparsers.add_parser("collections", help="MongoDB collection inspection")
    collections_subparsers = collections_parser.add_subparsers(dest="collections_action", help="Collection actions")
    
    # Collection count
    collections_count_parser = collections_subparsers.add_parser("count", help="Count documents in MongoDB collection")
    collections_count_parser.add_argument("-c", "--collection", required=True, help="Collection name to count")
    
    # Collection query
    collections_query_parser = collections_subparsers.add_parser("query", help="Query documents from MongoDB collection")
    collections_query_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection)")
    collections_query_parser.add_argument("-l", "--limit", type=int, default=100, help="Number of documents to return (default: 100)")
    collections_query_parser.add_argument("-f", "--filter", help="MongoDB filter as JSON string (default: {})")
    collections_query_parser.add_argument("-p", "--projection", help="Fields to include as JSON string (default: all fields)")
    
    # Collection list
    collections_list_parser = collections_subparsers.add_parser("list", help="List all collections in a database")
    collections_list_parser.add_argument("-d", "--database", default="test", help="Database name (default: test)")
    
    # Collection TTL management
    collections_ttl_parser = collections_subparsers.add_parser("ttl", help="Manage TTL (time-to-live) for MongoDB collection")
    collections_ttl_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection or just collection)")
    collections_ttl_parser.add_argument("-s", "--seconds", type=int, help="TTL in seconds. Omit to remove TTL.")
    collections_ttl_parser.add_argument("-f", "--field", default="_ts", help="Field to set TTL on (default: _ts)")
    
    # Collection insertOne
    collections_insert_parser = collections_subparsers.add_parser("insertOne", help="Insert a document into MongoDB collection")
    collections_insert_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection)")
    collections_insert_parser.add_argument("-d", "--document", help="Document as JSON string (default: generates test doc with timestamp)")
    
    # Collection deleteOne
    collections_delete_parser = collections_subparsers.add_parser("deleteOne", help="Delete a document from MongoDB collection")
    collections_delete_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection)")
    collections_delete_parser.add_argument("-f", "--filter", help="MongoDB filter as JSON string (default: deletes first document)")
    
    # Collection index management
    collections_index_parser = collections_subparsers.add_parser("index", help="Manage indexes on MongoDB collection")
    collections_index_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection)")
    collections_index_parser.add_argument("-k", "--keys", help="Index keys as JSON (e.g., '{\"field\": 1}' or '{\"a\": 1, \"b\": -1}')")
    collections_index_parser.add_argument("-u", "--unique", action="store_true", help="Create unique index")
    collections_index_parser.add_argument("-n", "--name", help="Index name (auto-generated if not specified)")
    collections_index_parser.add_argument("--drop", help="Drop index by name")
    collections_index_parser.add_argument("--list", action="store_true", help="List all indexes on the collection")
    
    # Collection preImages (changeStreamPreAndPostImages)
    collections_preimages_parser = collections_subparsers.add_parser("preImages", help="Enable/disable changeStreamPreAndPostImages on collection")
    collections_preimages_parser.add_argument("-c", "--collection", required=True, help="Collection name (format: database.collection)")
    collections_preimages_parser.add_argument("--enable", action="store_true", help="Enable pre and post images")
    collections_preimages_parser.add_argument("--disable", action="store_true", help="Disable pre and post images")
    
    # Materialized Views
    materialized_views_parser = subparsers.add_parser("materialized_views", help="Materialized view management (collection + stream processor)")
    materialized_views_subparsers = materialized_views_parser.add_subparsers(dest="materialized_views_action", help="Materialized view actions")
    
    # Create materialized view
    materialized_views_create_parser = materialized_views_subparsers.add_parser("create", help="Create a materialized view (collection + processor)")
    materialized_views_create_parser.add_argument("view_name", help="Name for both the collection and processor")
    materialized_views_create_parser.add_argument("-d", "--database", required=True, help="Database name for the materialized view collection")
    materialized_views_create_parser.add_argument("-f", "--file", required=True, help="Processor JSON file path")
    
    # List materialized views
    materialized_views_list_parser = materialized_views_subparsers.add_parser("list", help="List all materialized views (collections with matching processors)")
    materialized_views_list_parser.add_argument("-d", "--database", help="Filter by database name (optional)")
    
    # Drop materialized view
    materialized_views_drop_parser = materialized_views_subparsers.add_parser("drop", help="Drop a materialized view (removes both collection and processor)")
    materialized_views_drop_parser.add_argument("view_name", help="Name of the materialized view to drop")
    materialized_views_drop_parser.add_argument("-d", "--database", help="Database name to limit search (optional)")
    
    # Global options
    parser.add_argument("--config", default="../../config.txt",
                       help="Atlas API configuration file path (default: ../../config.txt)")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    try:
        # Smart config file detection - look for config.txt in common locations
        config_file = args.config
        if config_file == "../config.txt":
            # Default path - try different locations
            possible_paths = ["../config.txt", "./config.txt", "config.txt"]
            for path in possible_paths:
                if Path(path).exists():
                    config_file = path
                    break
        
        api = AtlasStreamProcessingAPI(config_file)
    except (FileNotFoundError, ValueError) as e:
        error_result = {
            "error": str(e),
            "message": "Please check your configuration file"
        }
        print(colorize_json(error_result))
        sys.exit(1)

    # Handle workspace management commands (both workspaces and instances commands)
    if args.command == "workspaces":
        if not args.workspace_action:
            workspaces_parser.print_help()
            sys.exit(1)
            
        if args.workspace_action == "list":
            result = api.list_instances()
            print(colorize_json(result))
            
        elif args.workspace_action == "create":
            result = api.create_instance(args.name, args.cloud_provider, args.region)
            print(colorize_json(result))
            
        elif args.workspace_action == "delete":
            result = api.delete_instance(args.name)
            print(colorize_json(result))
            
        elif args.workspace_action == "details":
            result = api.get_instance_details(args.name)
            print(colorize_json(result))
            
        elif args.workspace_action == "connections":
            if not args.connections_action:
                workspaces_connections_parser.print_help()
                sys.exit(1)
                
            if args.connections_action == "create":
                # Adjust path since we're running from tools/sp/ directory
                config_dir = "../../" if Path("../../connections").exists() else "./"
                result = create_connections(api, config_dir)
                print(colorize_json(result))
                
            elif args.connections_action == "list":
                timestamp = datetime.now(timezone.utc).isoformat()
                connections = api.list_connections()
                result = {
                    "timestamp": timestamp,
                    "operation": "list_connections",
                    "summary": {"total": len(connections)},
                    "connections": connections
                }
                print(colorize_json(result))
                
            elif args.connections_action == "delete":
                result = api.delete_connection(args.connection_name)
                print(colorize_json(result))
                
            # ...test feature removed...
        
        return
                
    # Handle instance management commands (legacy alias)
    elif args.command == "instances":
        if not args.instance_action:
            instances_parser.print_help()
            sys.exit(1)
            
        if args.instance_action == "list":
            result = api.list_instances()
            print(colorize_json(result))
            
        elif args.instance_action == "create":
            result = api.create_instance(args.name, args.cloud_provider, args.region)
            print(colorize_json(result))
            
        elif args.instance_action == "delete":
            result = api.delete_instance(args.name)
            print(colorize_json(result))
            
        elif args.instance_action == "details":
            result = api.get_instance_details(args.name)
            print(colorize_json(result))
            
        elif args.instance_action == "connections":
            if not args.connections_action:
                instances_connections_parser.print_help()
                sys.exit(1)
                
            if args.connections_action == "create":
                # Adjust path since we're running from tools/sp/ directory
                config_dir = "../../" if Path("../../connections").exists() else "./"
                result = create_connections(api, config_dir)
                print(colorize_json(result))
                
            elif args.connections_action == "list":
                result = api.list_connections()
                print(colorize_json(result))
                
            elif args.connections_action == "delete":
                result = api.delete_connection(args.connection_name)
                print(colorize_json(result))
                
            # ...test feature removed...
            
        return

    # Handle processor management commands
    if args.command == "processors":
        if not args.processor_action:
            processors_parser.print_help()
            sys.exit(1)
        
        if args.processor_action == "create":
            # Adjust path since we're running from tools/sp/ directory  
            config_dir = "../../" if Path("../../processors").exists() else "./"
            result = create_processors(api, config_dir, args.processor)
            print(colorize_json(result))
            
        elif args.processor_action == "list":
            if args.processor:
                # List specific processor
                result = api.get_single_processor_status(args.processor)
            else:
                # List all processors with full details (always verbose)
                processors = api.list_processors(verbose=True)
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "list",
                    "summary": {"total": len(processors), "success": len(processors), "failed": 0},
                    "processors": processors
                }
                    
            print(colorize_json(result))
            
        elif args.processor_action == "stats":
            verbose = getattr(args, 'verbose', False)
            if args.processor:
                # Stats for specific processor
                result = api.get_single_processor_stats(args.processor, verbose=verbose)
            else:
                # Stats for all processors
                result = api.get_processor_stats(verbose=verbose)
            print(colorize_json(result))
                
        elif args.processor_action == "start":
            if args.processor:
                # Start specific processor
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "start",
                    "summary": {"total": 1, "success": 0, "failed": 0},
                    "processors": []
                }
                
                # Determine tier
                tier = None
                if args.auto:
                    tier = api.analyze_processor_complexity(args.processor)
                    print(f"Auto-selected tier {tier} for processor {args.processor}")
                elif args.tier:
                    tier = args.tier
                
                proc_result = api.start_processor(args.processor, tier)
                result["processors"].append(proc_result)
                if proc_result["status"] == "started":
                    result["summary"]["success"] += 1
                else:
                    result["summary"]["failed"] += 1
            else:
                # Start all processors
                processors = api.list_processors()
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "start",
                    "summary": {"total": len(processors), "success": 0, "failed": 0},
                    "processors": []
                }
                
                # Use all-tier if specified, otherwise no tier (unless auto)
                tier_for_all = getattr(args, 'all_tier', None)
                
                for processor in processors:
                    # Determine tier for each processor
                    if args.auto:
                        tier = api.analyze_processor_complexity(processor["name"])
                        print(f"Auto-selected tier {tier} for processor {processor['name']}")
                    else:
                        tier = tier_for_all
                        
                    proc_result = api.start_processor(processor["name"], tier)
                    result["processors"].append(proc_result)
                    if proc_result["status"] == "started":
                        result["summary"]["success"] += 1
                    else:
                        result["summary"]["failed"] += 1
            
            print(colorize_json(result))
        
        elif args.processor_action == "stop":
            if args.processor:
                # Stop specific processor
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "stop",
                    "summary": {"total": 1, "success": 0, "failed": 0},
                    "processors": []
                }
                proc_result = api.stop_processor(args.processor)
                result["processors"].append(proc_result)
                if proc_result["status"] == "stopped":
                    result["summary"]["success"] += 1
                else:
                    result["summary"]["failed"] += 1
            else:
                # Stop all processors
                processors = api.list_processors()
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "stop",
                    "summary": {"total": len(processors), "success": 0, "failed": 0},
                    "processors": []
                }
                
                for processor in processors:
                    proc_result = api.stop_processor(processor["name"])
                    result["processors"].append(proc_result)
                    if proc_result["status"] == "stopped":
                        result["summary"]["success"] += 1
                    else:
                        result["summary"]["failed"] += 1
            
            print(colorize_json(result))
        
        elif args.processor_action == "restart":
            if args.processor:
                # Restart specific processor
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "restart",
                    "summary": {"total": 3, "success": 0, "failed": 0},
                    "processors": []
                }
                
                # Stop processor
                proc_result = api.stop_processor(args.processor)
                result["processors"].append(proc_result)
                if proc_result["status"] == "stopped":
                    result["summary"]["success"] += 1
                else:
                    result["summary"]["failed"] += 1
                
                # Wait a bit
                time.sleep(2)
                
                # Update processor definition from JSON file (NEW STEP)
                try:
                    processor_file = Path(f"processors/{args.processor}.json")
                    if processor_file.exists():
                        with open(processor_file, 'r') as f:
                            processor_data = json.load(f)
                        
                        pipeline = processor_data.get("pipeline", [])
                        options = processor_data.get("options")
                        
                        if pipeline:
                            update_result = api.update_processor(
                                processor_name=args.processor,
                                pipeline=pipeline,
                                options=options
                            )
                            result["processors"].append(update_result)
                            if update_result["status"] == "updated":
                                result["summary"]["success"] += 1
                            else:
                                result["summary"]["failed"] += 1
                        else:
                            update_result = {
                                "name": args.processor,
                                "operation": "update_processor",
                                "status": "failed",
                                "message": "No pipeline found in JSON file"
                            }
                            result["processors"].append(update_result)
                            result["summary"]["failed"] += 1
                    else:
                        update_result = {
                            "name": args.processor,
                            "operation": "update_processor", 
                            "status": "skipped",
                            "message": f"JSON file not found: {processor_file}"
                        }
                        result["processors"].append(update_result)
                        result["summary"]["success"] += 1  # Don't count as failure since processor can exist without local JSON
                        
                except Exception as e:
                    update_result = {
                        "name": args.processor,
                        "operation": "update_processor",
                        "status": "failed", 
                        "message": f"Error updating processor: {str(e)}"
                    }
                    result["processors"].append(update_result)
                    result["summary"]["failed"] += 1
                
                # Start processor
                tier = None
                if args.auto:
                    tier = api.analyze_processor_complexity(args.processor)
                    print(f"Auto-selected tier {tier} for processor {args.processor}")
                elif args.tier:
                    tier = args.tier
                    
                proc_result = api.start_processor(args.processor, tier)
                result["processors"].append(proc_result)
                if proc_result["status"] == "started":
                    result["summary"]["success"] += 1
                else:
                    result["summary"]["failed"] += 1
            else:
                # Restart all processors
                processors = api.list_processors()
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "restart",
                    "summary": {"total": len(processors) * 2, "success": 0, "failed": 0},
                    "processors": []
                }
                
                # Use all-tier if specified, otherwise no tier (unless auto)
                tier_for_all = getattr(args, 'all_tier', None)
                
                # Stop all processors
                for processor in processors:
                    proc_result = api.stop_processor(processor["name"])
                    result["processors"].append(proc_result)
                    if proc_result["status"] == "stopped":
                        result["summary"]["success"] += 1
                    else:
                        result["summary"]["failed"] += 1
                
                # Wait a bit
                time.sleep(2)
                
                # Start all processors
                for processor in processors:
                    # Determine tier for each processor
                    if args.auto:
                        tier = api.analyze_processor_complexity(processor["name"])
                        print(f"Auto-selected tier {tier} for processor {processor['name']}")
                    else:
                        tier = tier_for_all
                        
                    proc_result = api.start_processor(processor["name"], tier)
                    result["processors"].append(proc_result)
                    if proc_result["status"] == "started":
                        result["summary"]["success"] += 1
                    else:
                        result["summary"]["failed"] += 1
            
            print(colorize_json(result))
        
        elif args.processor_action == "drop":
            # Handle -p/--processor option first
            if args.processor:
                target_processor = args.processor
            elif args.processor_name:
                target_processor = args.processor_name
            else:
                target_processor = None
                
            if target_processor:
                # Drop a specific processor
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "drop",
                    "summary": {"total": 1, "success": 0, "failed": 0},
                    "processors": []
                }
                
                proc_result = api.delete_processor(target_processor)
                result["processors"].append(proc_result)
                if proc_result.get("status") == "deleted":
                    result["summary"]["success"] += 1
                else:
                    result["summary"]["failed"] += 1
                
                print(colorize_json(result))
                
            elif args.all:
                # Drop all processors
                processors = api.list_processors()
                timestamp = datetime.now(timezone.utc).isoformat()
                result = {
                    "timestamp": timestamp,
                    "operation": "drop_all",
                    "summary": {"total": len(processors), "success": 0, "failed": 0},
                    "processors": []
                }
                
                for processor in processors:
                    proc_result = api.delete_processor(processor["name"])
                    result["processors"].append(proc_result)
                    if proc_result.get("status") == "deleted":
                        result["summary"]["success"] += 1
                    else:
                        result["summary"]["failed"] += 1
                
                print(colorize_json(result))
                
            else:
                # No processor name and no --all flag
                error_result = {
                    "error": "Must specify either a processor name or use --all flag",
                    "usage": "sp processors drop <processor_name> OR sp processors drop --all OR sp processors drop -p <processor_name>",
                    "examples": [
                        "sp processors drop solar_simple_processor",
                        "sp processors drop -p solar_simple_processor",
                        "sp processors drop --processor solar_simple_processor",
                        "sp processors drop --all"
                    ]
                }
                print(colorize_json(error_result))
                sys.exit(1)
        
        # ...test feature removed...
        
        elif args.processor_action == "tier-advise":
            # Handle tier analysis/advice
            timestamp = datetime.now(timezone.utc).isoformat()
            
            if args.processor:
                # Analyze specific processor
                try:
                    detailed_analysis = api.analyze_processor_complexity_detailed(args.processor)
                    result = {
                        "timestamp": timestamp,
                        "operation": "tier-advise",
                        "processor": args.processor,
                        "recommended_tier": detailed_analysis["recommended_tier"],
                        "analysis": detailed_analysis["analysis"],
                        "reasoning": detailed_analysis["reasoning"],
                        "status": "success"
                    }
                except Exception as e:
                    result = {
                        "timestamp": timestamp,
                        "operation": "tier-advise",
                        "processor": args.processor,
                        "status": "error",
                        "message": str(e)
                    }
                    
            elif args.all:
                # Analyze all processors
                try:
                    processors = api.list_processors()
                    result = {
                        "timestamp": timestamp,
                        "operation": "tier-advise",
                        "summary": {"total": len(processors), "success": 0, "failed": 0},
                        "processors": []
                    }
                    
                    for processor in processors:
                        processor_name = processor.get("name")
                        try:
                            detailed_analysis = api.analyze_processor_complexity_detailed(processor_name)
                            proc_result = {
                                "name": processor_name,
                                "current_status": processor.get("state", processor.get("status", "unknown")),
                                "current_tier": processor.get("tier", "unknown"), 
                                "recommended_tier": detailed_analysis["recommended_tier"],
                                "analysis": detailed_analysis["analysis"],
                                "reasoning": detailed_analysis["reasoning"],
                                "status": "success"
                            }
                            result["summary"]["success"] += 1
                        except Exception as e:
                            proc_result = {
                                "name": processor_name,
                                "status": "error",
                                "message": str(e)
                            }
                            result["summary"]["failed"] += 1
                            
                        result["processors"].append(proc_result)
                        
                except Exception as e:
                    result = {
                        "timestamp": timestamp,
                        "operation": "tier-advise",
                        "status": "error", 
                        "message": f"Failed to list processors: {str(e)}"
                    }
            else:
                # No processor specified and --all not used
                result = {
                    "timestamp": timestamp,
                    "operation": "tier-advise",
                    "status": "error",
                    "message": "Please specify a processor with -p or use --all to analyze all processors"
                }
            
            print(colorize_json(result))
        
        elif args.processor_action == "profile":
            # Handle processor profiling
            timestamp = datetime.now(timezone.utc).isoformat()
            
            # Determine which processors to profile
            if args.processor:
                processor_names = [args.processor]
            elif args.all:
                try:
                    processors = api.list_processors()
                    processor_names = [p.get("name") for p in processors if p.get("state") == "STARTED"]
                except Exception as e:
                    result = {
                        "timestamp": timestamp,
                        "operation": "profile",
                        "status": "error",
                        "message": f"Failed to list processors: {str(e)}"
                    }
                    print(colorize_json(result))
                    return
            else:
                result = {
                    "timestamp": timestamp,
                    "operation": "profile",
                    "status": "error",
                    "message": "Please specify a processor with -p or use --all to profile all running processors"
                }
                print(colorize_json(result))
                return
            
            # Parse metrics
            requested_metrics = [m.strip() for m in args.metrics.split(',')]
            
            # Load thresholds if specified
            thresholds = {}
            if args.thresholds:
                try:
                    with open(args.thresholds, 'r') as f:
                        thresholds = json.load(f)
                except Exception as e:
                    print(f"Warning: Could not load thresholds file: {e}")
            
            try:
                # Run profiling
                if args.continuous:
                    print(f"Starting continuous profiling of {len(processor_names)} processor(s). Press Ctrl+C to stop...")
                    result = api.profile_processors_continuous(processor_names, args.interval, requested_metrics, thresholds)
                else:
                    print(f"Profiling {len(processor_names)} processor(s) for {args.duration} seconds (sampling every {args.interval}s)...")
                    result = api.profile_processors(processor_names, args.duration, args.interval, requested_metrics, thresholds)
                
                # Save to output file if specified
                if args.output:
                    with open(args.output, 'w') as f:
                        json.dump(result, f, indent=2)
                    print(f"Results saved to {args.output}")
                
                print(colorize_json(result))
                
            except KeyboardInterrupt:
                print("\nProfiling interrupted by user")
                return
            except Exception as e:
                result = {
                    "timestamp": timestamp,
                    "operation": "profile",
                    "status": "error",
                    "message": str(e)
                }
                print(colorize_json(result))
        
        elif args.processor_action == "schema":
            # Sample processor output to show schema
            result = api.sample_processor(args.processor_name, num_samples=args.num_samples)
            print(colorize_json(result))
        
        return

    # Handle collections management commands
    if args.command == "collections":
        if not args.collections_action:
            collections_parser.print_help()
            sys.exit(1)
        
        if args.collections_action == "count":
            if not args.collection:
                collections_count_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                # Default to 'test' database if not specified
                database = 'test'
                collection = args.collection
            
            # Count documents in the specified collection
            result = api.check_collection(database, collection)
            print(colorize_json(result))
        
        elif args.collections_action == "query":
            # Query documents from collection
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                database = 'test'
                collection = args.collection
            
            # Parse filter and projection if provided
            filter_doc = {}
            if args.filter:
                filter_doc = json.loads(args.filter)
            
            projection = None
            if args.projection:
                projection = json.loads(args.projection)
            
            result = api.query_collection(database, collection, filter_doc, projection, args.limit)
            print(colorize_json(result))
        
        elif args.collections_action == "list":
            # List all collections in the specified database
            result = api.list_database_collections(args.database)
            print(colorize_json(result))
        
        elif args.collections_action == "ttl":
            if not args.collection:
                collections_ttl_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                # Default to 'test' database if not specified
                database = 'test'
                collection = args.collection
            
            # Manage TTL for the specified collection
            result = api.manage_collection_ttl(database, collection, args.seconds, args.field)
            print(colorize_json(result))
        
        elif args.collections_action == "insertOne":
            if not args.collection:
                collections_insert_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                print(colorize_json({"status": "error", "message": "Collection must be in format: database.collection"}))
                sys.exit(1)
            
            # Parse document or generate a test document
            if args.document:
                try:
                    document = json.loads(args.document)
                except json.JSONDecodeError as e:
                    print(colorize_json({"status": "error", "message": f"Invalid JSON document: {e}"}))
                    sys.exit(1)
            else:
                # Generate a test document with timestamp (datetime imported at top)
                document = {
                    "test": True,
                    "message": "Test document inserted by sp tool",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "_ts": datetime.now(timezone.utc)
                }
            
            result = api.insert_one(database, collection, document)
            print(colorize_json(result))
        
        elif args.collections_action == "deleteOne":
            if not args.collection:
                collections_delete_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                print(colorize_json({"status": "error", "message": "Collection must be in format: database.collection"}))
                sys.exit(1)
            
            # Parse filter or default to empty (deletes first document)
            filter_doc = {}
            if args.filter:
                try:
                    filter_doc = json.loads(args.filter)
                except json.JSONDecodeError as e:
                    print(colorize_json({"status": "error", "message": f"Invalid JSON filter: {e}"}))
                    sys.exit(1)
            
            result = api.delete_one(database, collection, filter_doc)
            print(colorize_json(result))
        
        elif args.collections_action == "index":
            if not args.collection:
                collections_index_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                print(colorize_json({"status": "error", "message": "Collection must be in format: database.collection"}))
                sys.exit(1)
            
            if args.list:
                # List all indexes
                result = api.list_indexes(database, collection)
            elif args.drop:
                # Drop an index by name
                result = api.drop_index(database, collection, args.drop)
            elif args.keys:
                # Create an index
                try:
                    keys = json.loads(args.keys)
                except json.JSONDecodeError as e:
                    print(colorize_json({"status": "error", "message": f"Invalid JSON keys: {e}"}))
                    sys.exit(1)
                result = api.create_index(database, collection, keys, unique=args.unique, name=args.name)
            else:
                # Default to list if no action specified
                result = api.list_indexes(database, collection)
            
            print(colorize_json(result))
        
        elif args.collections_action == "preImages":
            if not args.collection:
                collections_preimages_parser.print_help()
                sys.exit(1)
            
            # Parse database and collection from the collection argument (format: database.collection)
            if '.' in args.collection:
                database, collection = args.collection.split('.', 1)
            else:
                print(colorize_json({"status": "error", "message": "Collection must be in format: database.collection"}))
                sys.exit(1)
            
            if args.disable:
                result = api.set_pre_post_images(database, collection, enabled=False)
            else:
                # Default to enable
                result = api.set_pre_post_images(database, collection, enabled=True)
            
            print(colorize_json(result))
        
        return

    # Handle materialized views management commands  
    if args.command == "materialized_views":
        if not args.materialized_views_action:
            materialized_views_parser.print_help()
            sys.exit(1)
        
        if args.materialized_views_action == "create":
            # Adjust path since we're running from tools/sp/ directory
            config_dir = "../../" if Path("../../processors").exists() else "./"
            processor_file = os.path.join(config_dir, "processors", args.file)
            
            # Create materialized view
            result = api.create_materialized_view(args.view_name, args.database, processor_file)
            print(colorize_json(result))
            
        elif args.materialized_views_action == "list":
            result = api.list_materialized_views(database=args.database)
            print(colorize_json(result))
            
        elif args.materialized_views_action == "drop":
            result = api.drop_materialized_view(args.view_name, database=args.database)
            print(colorize_json(result))
        
        return

    # If we get here, an unknown command was provided
    parser.print_help()
    sys.exit(1)


if __name__ == "__main__":
    main()
